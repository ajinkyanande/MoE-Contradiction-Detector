model:
  gating_network:
    # Base models for embedding sentence for the gating network
    base_encoder_model: "microsoft/MiniLM-L12-H384-uncased"

    # Layers to freeze for the base model during training
    # positive value means from the start, negative value means from the end
    # "all" is freezing all layers
    freeze_layers: -2

    # Classifier to select experts
    hidden_dims: [256, 128]
    dropout: 0.1

  experts_network:
    # Base models for embedding sentence for the expert network
    base_encoder_model: "microsoft/MiniLM-L12-H384-uncased"

    # Number of experts and top_k experts to activate
    num_experts: 4
    top_k: 2

    # Layers to freeze for the base model during training
    # positive value means from the start, negative value means from the end
    # "all" is freezing all layers
    freeze_layers: -2

    # Classifier to predict the label
    classifier_hidden_dims: [256, 128]
    classifier_dropout: 0.1

data:
  hf_dataset: "stanfordnlp/snli"
  training_subset: 0.1
  validation_subset: 0.5

training:
  batch_size: 128
  epochs: 10

  # Previous model / checkpoint to continue training from
  # model params are ignored if this is set
  pretrained_model_path: null

  lr:
    scheduler_type: null
    lr_start: 0.001

    cosine_annealing:
      lr_end: 0.0001

    reduce_on_plateau:
      patience: 3
      threshold: 0.0001
      factor: 0.1

  val_steps: 2000
  gradient_clip: 1.0
  mixed_precision: "16-mixed"

inference:
  best_model_path: "models/best_model.pth"

logging:
  log_dir: "logs"
  checkpoint_dir: "logs/checkpoints"
  mlflow_uri: "http://localhost:5000"
  mlflow_experiment_name: "default"
