model:
  # Previous model / checkpoint to continue training from
  # model params below are ignored if this is set
  pretrained_model_path: null

  gating_network:
    tokenizer:
      model: "microsoft/MiniLM-L12-H384-uncased"
      max_length: 128

    # Base models for embedding sentence for the gating network
    base_encoder_model: "microsoft/MiniLM-L12-H384-uncased"

    # Layers to freeze for the base model during training
    # positive value means from the start, negative value means from the end
    # "all" is freezing all layers
    freeze_layers: -3

    # Classifier to select experts
    hidden_dims: [256, 128, 64]
    dropout: 0.1

  experts_network:
    tokenizer:
      model: "microsoft/MiniLM-L12-H384-uncased"
      max_length: 128

    # Base models for embedding sentence for the expert network
    base_encoder_model: "microsoft/MiniLM-L12-H384-uncased"

    # Layers to freeze for the base model during training
    # positive value means from the start, negative value means from the end
    # "all" is freezing all layers
    freeze_layers: -3

    # Number of experts and top_k experts to activate
    num_experts: 4
    top_k: 2

    # LoRA parameters for the expert models
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    lora_target_modules: ["query", "key", "value"]

    # Classifier to predict the label
    classifier_hidden_dims: [256, 128, 64]
    classifier_dropout: 0.1

data:
  hf_dataset: "stanfordnlp/snli"
  training_subset: 1.0
  validation_subset: 1.0

training:
  batch_size: 64
  epochs: 25

  lr:
    scheduler_type: "cosine_annealing_warm_restarts"
    lr_start: 0.001

    cosine_annealing:
      lr_end: 0.00001
      T_max: 25

    cosine_annealing_warm_restarts:
      lr_end: 0.00001
      T_0: 5
      T_mult: 2

    reduce_on_plateau:
      patience: 2
      threshold: 0.0001
      factor: 0.5

  # Encourages diversity in the expert selection
  diversity_loss_weight: 0.2

  gradient_clip: 1.0
  mixed_precision: "16-mixed"

  # Accumulate gradients for x batches before updating the weights
  accumulate_grad_batches: 16

inference:
  best_model_path: "models/best_model.pth"

logging:
  log_dir: "logs"
  checkpoint_dir: "logs/checkpoints"
